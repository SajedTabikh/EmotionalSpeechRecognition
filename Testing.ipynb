{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing The Speech Emotion Recognition Model**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr(data, frame_length, hop_length):\n",
    "    zcr = librosa.feature.zero_crossing_rate(data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "def rmse(data, frame_length=2048, hop_length=512):\n",
    "    # Use 'y' keyword argument for the audio data\n",
    "    rms = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(rms)\n",
    "\n",
    "def mfcc(data, sr, frame_length=2048, hop_length=512, flatten=True):\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(mfcc.T) if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n",
    "    result = np.array([])\n",
    "    result = np.hstack((result,\n",
    "                        zcr(data, frame_length, hop_length),\n",
    "                        rmse(data, frame_length, hop_length),\n",
    "                        mfcc(data, sr, frame_length, hop_length)\n",
    "                       ))\n",
    "    return result\n",
    "\n",
    "def get_features(path, duration=2.5, offset=0.6):\n",
    "    data, sr = librosa.load(path, duration=duration, offset=offset)\n",
    "    aud = extract_features(data, sr)\n",
    "    audio = np.array(aud)\n",
    "\n",
    "    # Assuming noise() and pitch() functions are defined elsewhere\n",
    "    noised_audio = noise(data)\n",
    "    aud2 = extract_features(noised_audio, sr)\n",
    "    audio = np.vstack((audio, aud2))\n",
    "\n",
    "    pitched_audio = pitch(data, sr)\n",
    "    aud3 = extract_features(pitched_audio, sr)\n",
    "    audio = np.vstack((audio, aud3))\n",
    "\n",
    "    pitched_audio1 = pitch(data, sr)\n",
    "    pitched_noised_audio = noise(pitched_audio1)\n",
    "    aud4 = extract_features(pitched_noised_audio, sr)\n",
    "    audio = np.vstack((audio, aud4))\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and compiled successfully\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Load the model architecture from the JSON file\n",
    "json_file = open('CNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the model weights\n",
    "loaded_model.load_weights(\"best_model1_weights.h5\")\n",
    "\n",
    "# Compile the loaded model\n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model loaded and compiled successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler and encoder loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda3\\envs\\gpuEnv\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "d:\\Conda3\\envs\\gpuEnv\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator OneHotEncoder from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved scaler\n",
    "with open('scaler2.pickle', 'rb') as f:\n",
    "    scaler2 = pickle.load(f)\n",
    "\n",
    "# Load the saved encoder\n",
    "with open('encoder2.pickle', 'rb') as f:\n",
    "    encoder2 = pickle.load(f)\n",
    "\n",
    "print(\"Scaler and encoder loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_feat(path):\n",
    "    d, s_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    res = extract_features(d)\n",
    "    result = np.array(res)\n",
    "    result = np.reshape(result, newshape=(1, 2376))  # Make sure the shape matches your model input\n",
    "    i_result = scaler2.transform(result)  # Use the loaded scaler\n",
    "    final_result = np.expand_dims(i_result, axis=2)\n",
    "    return final_result\n",
    "\n",
    "def prediction(path1):\n",
    "    res = get_predict_feat(path1)\n",
    "    predictions = loaded_model.predict(res)\n",
    "    y_pred = encoder2.inverse_transform(predictions)\n",
    "    print(\"Predicted Emotion: \", y_pred[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction(\"RAVDESS Emotional speech audio/Actor_06/03-01-08-02-01-01-06.wav\") # It should be \"Suprised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction(\"RAVDESS Emotional speech audio/Actor_18/03-01-04-01-01-02-18.wav\") # It should be \"Sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction(\"Surrey Audio-Visual Expressed Emotion (SAVEE)/ALL/DC_f12.wav\") # It should be \"Fear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
